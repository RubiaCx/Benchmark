import torch
import triton
import triton.language as tl

print("ğŸ” å¼€å§‹æµ‹è¯•...")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"Tritonç‰ˆæœ¬: {triton.__version__}")
print(f"CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")

# æµ‹è¯•åŸºæœ¬çš„tritonåŠŸèƒ½
@triton.jit
def simple_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    output = x * 2
    tl.store(output_ptr + offsets, output, mask=mask)

def test_basic_triton():
    print("âœ… æµ‹è¯•åŸºæœ¬TritonåŠŸèƒ½...")
    size = 1024
    x = torch.randn(size, device='cuda')
    output = torch.empty_like(x)
    
    grid = lambda meta: (triton.cdiv(size, meta['BLOCK_SIZE']),)
    simple_kernel[grid](x, output, size, BLOCK_SIZE=256)
    
    expected = x * 2
    if torch.allclose(output, expected):
        print("âœ… åŸºæœ¬Tritonæµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âŒ åŸºæœ¬Tritonæµ‹è¯•å¤±è´¥")
        return False

def test_small_attention():
    print("ğŸ§ª æµ‹è¯•å°è§„æ¨¡attention...")
    try:
        # å¯¼å…¥attentionå‡½æ•°
        from attn_triton import attention
        
        # åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ•°æ®
        batch, heads, seq_len, head_dim = 1, 2, 128, 64
        
        q = torch.randn((batch, heads, seq_len, head_dim), dtype=torch.float16, device='cuda')
        k = torch.randn((batch, heads, seq_len, head_dim), dtype=torch.float16, device='cuda')
        v = torch.randn((batch, heads, seq_len, head_dim), dtype=torch.float16, device='cuda')
        
        print(f"  - è¾“å…¥å½¢çŠ¶: {q.shape}")
        print(f"  - è®¾å¤‡: {q.device}")
        print(f"  - æ•°æ®ç±»å‹: {q.dtype}")
        
        # æ‰§è¡Œattention
        result = attention(q, k, v, causal=True, sm_scale=0.125)
        print(f"âœ… å°è§„æ¨¡attentionæµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {result.shape}")
        return True
        
    except Exception as e:
        print(f"âŒ å°è§„æ¨¡attentionæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_usage():
    print("ğŸ’¾ æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨...")
    torch.cuda.empty_cache()
    print(f"  - æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"  - å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")
    print(f"  - å·²ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.1f} GB")

if __name__ == "__main__":
    try:
        test_memory_usage()
        
        if test_basic_triton():
            if test_small_attention():
                print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            else:
                print("âš ï¸  attentionæµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é…ç½®")
        else:
            print("âš ï¸  åŸºæœ¬TritonåŠŸèƒ½æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        test_memory_usage() 